"""Utilities to load the RadioML 2016.10a dataset stored as text files."""

from __future__ import annotations

import numpy as np
from pathlib import Path
from typing import Dict, List, Sequence, Tuple

Array = np.ndarray

SAMPLES_PER_EXAMPLE = 128


def _read_complex_samples(path: Path, *, limit_examples: int | None = None) -> np.ndarray:
    """Parse a RadioML text file into a complex-valued array.

    The helper streams the raw text into a NumPy vector using ``np.fromstring`` to
    keep parsing fast even for the multi-megabyte files. When ``limit_examples`` is
    provided only the leading examples are retained, which prevents unnecessary
    parsing when callers only need a subset of each file.
    """

    text = path.read_text()
    cleaned = text.replace("(", " ").replace(")", " ")
    complex_values = np.fromstring(cleaned, dtype=np.complex64, sep=" ")

    if complex_values.size == 0:
        raise ValueError(f"No complex samples could be parsed from {path}")

    if limit_examples is not None:
        needed = limit_examples * SAMPLES_PER_EXAMPLE
        if complex_values.size >= needed:
            complex_values = complex_values[:needed]

    # Drop any trailing partial example that may remain due to text formatting.
    remainder = complex_values.size % SAMPLES_PER_EXAMPLE
    if remainder:
        complex_values = complex_values[:-remainder]

    if complex_values.size == 0:
        raise ValueError(
            f"After trimming partial frames there were no samples left in {path}"
        )

    return complex_values.reshape(-1, SAMPLES_PER_EXAMPLE)


def load_rml2016a(
    root: Path | str = Path("data/raw/RML2016.10a"),
    *,
    snrs: Sequence[int] | None = None,
    modulations: Sequence[str] | None = None,
    max_examples_per_class: int | None = None,
) -> Tuple[Array, Array, Array, List[str]]:
    """Load the dataset from the on-disk text files.

    Parameters
    ----------
    root:
        Directory containing subdirectories named after each modulation, each holding
        text files named ``snr_<value>.txt`` generated by the download script.
    snrs:
        Optional iterable selecting which SNRs to include. If ``None`` all SNRs are used.
    modulations:
        Optional iterable of modulation names to include. Comparison is case-insensitive.
    max_examples_per_class:
        When provided, the loader will truncate each modulation/SNR pair to the desired
        number of examples. This is useful for quick experiments.

    Returns
    -------
    samples: np.ndarray
        Array shaped ``(N, 2, 128)`` with in-phase and quadrature components.
    labels: np.ndarray
        Integer encoded modulation labels for each example.
    snr_values: np.ndarray
        Signal-to-noise ratio for each example.
    label_names: list[str]
        Names of the modulation classes in label index order.
    """

    root = Path(root)
    if not root.exists():
        raise FileNotFoundError(
            f"Dataset directory {root} not found. Did you run the download script?"
        )

    snr_set = set(snrs) if snrs is not None else None
    modulation_set = {m.upper() for m in modulations} if modulations is not None else None

    samples: List[np.ndarray] = []
    labels: List[int] = []
    snr_values: List[int] = []

    modulation_to_index: Dict[str, int] = {}

    for modulation_dir in sorted(root.iterdir()):
        if not modulation_dir.is_dir():
            continue
        modulation_name = modulation_dir.name
        if modulation_set is not None and modulation_name.upper() not in modulation_set:
            continue
        label_index = modulation_to_index.setdefault(modulation_name, len(modulation_to_index))

        for file_path in sorted(modulation_dir.glob("snr_*.txt")):
            try:
                snr = int(file_path.stem.split("_")[1])
            except (IndexError, ValueError) as exc:  # pragma: no cover - guard for corrupted files
                raise ValueError(f"Unexpected filename format: {file_path}") from exc
            if snr_set is not None and snr not in snr_set:
                continue

            iq_samples = _read_complex_samples(
                file_path, limit_examples=max_examples_per_class
            )
            if max_examples_per_class is not None:
                iq_samples = iq_samples[:max_examples_per_class]
            if iq_samples.size == 0:
                continue

            stacked = np.stack((iq_samples.real, iq_samples.imag), axis=1).astype(np.float32)
            samples.append(stacked)
            labels.extend([label_index] * stacked.shape[0])
            snr_values.extend([snr] * stacked.shape[0])

    if not samples:
        raise RuntimeError("No samples were loaded; please check the filters provided.")

    sample_array = np.concatenate(samples, axis=0)
    label_array = np.asarray(labels, dtype=np.int64)
    snr_array = np.asarray(snr_values, dtype=np.int32)
    label_names = [name for name, _ in sorted(modulation_to_index.items(), key=lambda kv: kv[1])]
    return sample_array, label_array, snr_array, label_names


class RMLDataset:
    """Simple dataset wrapper providing train/validation splits."""

    def __init__(
        self,
        root: Path | str = Path("data/raw/RML2016.10a"),
        *,
        snrs: Sequence[int] | None = None,
        modulations: Sequence[str] | None = None,
        max_examples_per_class: int | None = None,
    ) -> None:
        samples, labels, snrs_array, label_names = load_rml2016a(
            root,
            snrs=snrs,
            modulations=modulations,
            max_examples_per_class=max_examples_per_class,
        )
        self.samples = samples
        self.labels = labels
        self.snrs = snrs_array
        self.label_names = label_names

    def train_val_split(self, val_fraction: float = 0.2, *, random_state: int = 0) -> tuple[dict, dict]:
        rng = np.random.default_rng(random_state)
        num_samples = self.samples.shape[0]
        indices = np.arange(num_samples)
        rng.shuffle(indices)
        split = int(num_samples * (1 - val_fraction))
        train_idx = indices[:split]
        val_idx = indices[split:]

        def subset(idx: np.ndarray) -> dict:
            return {
                "samples": self.samples[idx],
                "labels": self.labels[idx],
                "snr": self.snrs[idx],
            }

        return subset(train_idx), subset(val_idx)
